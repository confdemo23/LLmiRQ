groupby_bigdata.q sometimes throws out of memory exception
I would get out of memory errors like the following when running groupby_bigdata.q.



  

    [junit] plan = /data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/scratchdir/plan38413.xml

    [junit] Exception in thread "Thread-15" java.lang.OutOfMemoryError: Java heap space

    [junit]     at java.util.Arrays.copyOf(Arrays.java:2882)

    [junit]     at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:100)

    [junit]     at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:390)

    [junit]     at java.lang.StringBuffer.append(StringBuffer.java:224)

    [junit]     at java.io.StringWriter.write(StringWriter.java:84)

    [junit]     at java.io.PrintWriter.newLine(PrintWriter.java:436)

    [junit]     at java.io.PrintWriter.println(PrintWriter.java:585)

    [junit]     at java.io.PrintWriter.println(PrintWriter.java:696)

    [junit]     at java.lang.Throwable.printStackTrace(Throwable.java:512)

    [junit]     at org.apache.hadoop.util.StringUtils.stringifyException(StringUtils.java:60)

    [junit]     at org.apache.hadoop.hive.ql.exec.ScriptOperator$StreamThread.run(ScriptOperator.java:561)

    [junit] Exception in thread "main" java.lang.OutOfMemoryError: Java heap space

    [junit]     at java.nio.HeapCharBuffer.&lt;init&gt;(HeapCharBuffer.java:39)

    [junit]     at java.nio.CharBuffer.allocate(CharBuffer.java:312)

    [junit]     at java.nio.charset.CharsetEncoder.isLegalReplacement(CharsetEncoder.java:319)

    [junit]     at java.nio.charset.CharsetEncoder.replaceWith(CharsetEncoder.java:267)

    [junit]     at java.nio.charset.CharsetEncoder.&lt;init&gt;(CharsetEncoder.java:186)

    [junit]     at java.nio.charset.CharsetEncoder.&lt;init&gt;(CharsetEncoder.java:209)

    [junit]     at sun.nio.cs.ISO_8859_1$Encoder.&lt;init&gt;(ISO_8859_1.java:116)

    [junit]     at sun.nio.cs.ISO_8859_1$Encoder.&lt;init&gt;(ISO_8859_1.java:113)

    [junit]     at sun.nio.cs.ISO_8859_1.newEncoder(ISO_8859_1.java:46)

    [junit]     at java.lang.StringCoding$StringEncoder.&lt;init&gt;(StringCoding.java:215)

    [junit]     at java.lang.StringCoding$StringEncoder.&lt;init&gt;(StringCoding.java:207)

    [junit]     at java.lang.StringCoding.encode(StringCoding.java:266)

    [junit]     at java.lang.String.getBytes(String.java:947)

    [junit]     at java.io.UnixFileSystem.getLength(Native Method)

    [junit]     at java.io.File.length(File.java:848)

    [junit]     at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.&lt;init&gt;(RawLocalFileSystem.java:375)

    [junit]     at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:359)

    [junit]     at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:245)

    [junit]     at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:643)

    [junit]     at org.apache.hadoop.hive.ql.exec.Utilities.clearMapRedWork(Utilities.java:114)

    [junit]     at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:680)

    [junit]     at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:936)

    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)

    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)

    [junit]     at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

    [junit] Traceback (most recent call last):

    [junit]   File "../data/scripts/dumpdata_script.py", line 6, in &lt;module&gt;

    [junit]     print 20000 * i + k




