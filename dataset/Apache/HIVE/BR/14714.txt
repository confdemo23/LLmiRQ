Avoid misleading "java.io.IOException: Stream closed" when shutting down HoS
After execute hive command with Spark, finishing the beeline session or
even switch the engine causes IOException. The following executed Ctrl-D to
finish the session but "!quit" or even "set hive.execution.engine=mr;" causes
the issue.
From HS2 log:



2016-09-06 16:15:12,291 WARN  org.apache.hive.spark.client.SparkClientImpl: [HiveServer2-Handler-Pool: Thread-106]: Timed out shutting down remote driver, interrupting...

2016-09-06 16:15:12,291 WARN  org.apache.hive.spark.client.SparkClientImpl: [Driver]: Waiting thread interrupted, killing child process.

2016-09-06 16:15:12,296 WARN  org.apache.hive.spark.client.SparkClientImpl: [stderr-redir-1]: Error in redirector thread.

java.io.IOException: Stream closed

        at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)

        at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)

        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)

        at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)

        at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)

        at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)

        at java.io.InputStreamReader.read(InputStreamReader.java:184)

        at java.io.BufferedReader.fill(BufferedReader.java:154)

        at java.io.BufferedReader.readLine(BufferedReader.java:317)

        at java.io.BufferedReader.readLine(BufferedReader.java:382)

        at org.apache.hive.spark.client.SparkClientImpl$Redirector.run(SparkClientImpl.java:617)

        at java.lang.Thread.run(Thread.java:745)


