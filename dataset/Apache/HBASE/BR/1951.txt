Stack overflow when calling HTable.checkAndPut() when deleting a lot of values
We get a stackoverflow when calling HTable.checkAndPut() from a map-reduce job though the client API after doing a large number of deletes.
Our mapred job is a periodic job (which extends TableMapper) that merges the versions for a value in a column into a new value/version and then deletes the older versions. This is because we use versions to store data so we can do append-only insertion. Our rows can have large/huge (from 1 till &gt; 1M) numbers of columns (aka key-values).
The problem seems to be that the org.apache.hadoop.hbase.regionserver.GetDeleteTracker.isDeleted() method is implemented with recursion but since Java has no tail recursion optimization, this fails for cases where the number of deletes that are being tracked is bigger than the stack size. I&amp;apos;m not sure why recursion is used here but it is not safe without tail-call optimization and it should be optimized into a simple loop.
I&amp;apos;ll attach the stacktrace.