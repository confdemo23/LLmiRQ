Connection leak during log splitting
Ran into an issue where Region server died with the following exception

2015-04-29 17:10:11,856 WARN  [nector@0.0.0.0:60030] mortbay.log - EXCEPTION

java.io.IOException: Too many open files

        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)

        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:241)

        at org.mortbay.jetty.nio.SelectChannelConnector$1.acceptChannel(SelectChannelConnector.java:75)

        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:686)

        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)

        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)

        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)

        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)



Realized that all the tcp sockets on the system were used out due to the regionserver trying to split the log and failing multiple times and leaving a connection open -

java.io.IOException: Got error for OP_READ_BLOCK, self=/10..99.3:50695, remote=/10.232.99.36:50010, for file /hbase/WALs/host1,60020,1425930917890-splitting/host1%2C60020%2C1425930917890.1429358890944, for pool BP-181199659-10.232.99.2-1411124363096 block 1074497051_756497

        at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432)

        at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397)

        at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)

        at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)

        at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)

        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:567)

        at org.apache.hadoop.hdfs.DFSInputStream.seekToNewSource(DFSInputStream.java:1446)

        at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:769)

        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:799)

        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:840)

        at java.io.DataInputStream.read(DataInputStream.java:100)

        at org.apache.hadoop.hbase.regionserver.wal.HLogFactory.createReader(HLogFactory.java:124)

        at org.apache.hadoop.hbase.regionserver.wal.HLogFactory.createReader(HLogFactory.java:91)

        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getReader(HLogSplitter.java:660)

        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getReader(HLogSplitter.java:569)

        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:282)

        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:225)

        at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:143)

        at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:82)

        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

        at java.lang.Thread.run(Thread.java:745)




