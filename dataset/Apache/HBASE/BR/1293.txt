hfile doesn&apos;t recycle decompressors
The Compression codec stuff from hadoop has the concept of recycling compressors and decompressors - this is because a compression codec uses "direct buffers" which reside outside the JVM regular heap space.  There is a risk that under heavy concurrent load we could run out of that &amp;apos;direct buffer&amp;apos; heap space in the JVM.
HFile does not call algorithm.returnDecompressor and returnCompressor.  We should fix that.
I found this bug via OOM crashes under jdk 1.7 - it appears to be partially due to the size of my cluster (200gb, 800 regions, 19 servers) and partially due to weaknesses in JVM 1.7.